# Kafka + Spark Streaming Pipeline

Pipeline de dados em tempo real usando Apache Kafka e Spark Streaming para processamento de dados IoT e atividade de usuÃ¡rios.

## ğŸ—ï¸ Arquitetura

```
IoT Sensors â†’ Kafka â†’ Spark Streaming â†’ PostgreSQL â†’ Grafana
User Events â†’ Kafka â†’ Spark Streaming â†’ PostgreSQL â†’ Grafana
```

## ğŸš€ CaracterÃ­sticas

- **Streaming em Tempo Real**: Processamento contÃ­nuo de dados
- **MÃºltiplas Fontes**: Sensores IoT + Atividade de usuÃ¡rios
- **AgregaÃ§Ãµes**: Janelas de tempo com watermarks
- **Alertas**: DetecÃ§Ã£o de anomalias em tempo real
- **VisualizaÃ§Ã£o**: Dashboards no Grafana
- **Escalabilidade**: Kafka particionado + Spark distribuÃ­do

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Apache Kafka**: Message broker para streaming
- **Apache Spark**: Processamento distribuÃ­do
- **PostgreSQL**: Armazenamento de dados processados
- **Grafana**: VisualizaÃ§Ã£o e dashboards
- **Docker**: ContainerizaÃ§Ã£o completa
- **Python**: Produtores e consumidores

## ğŸ“¦ Componentes

### Produtores de Dados
- **IoT Data Generator**: Simula sensores (temperatura, umidade, pressÃ£o)
- **User Activity Generator**: Simula atividade web (login, compras, navegaÃ§Ã£o)

### Processamento Spark
- **Real-time Processing**: TransformaÃ§Ãµes em tempo real
- **Windowed Aggregations**: MÃ©tricas por janelas de tempo
- **Alerting**: DetecÃ§Ã£o de temperaturas altas e bateria baixa
- **Data Enrichment**: ConversÃµes e categorizaÃ§Ãµes

### Armazenamento
- **PostgreSQL**: Dados processados e agregaÃ§Ãµes
- **Checkpointing**: Garantia de exactly-once processing

## ğŸš€ Como Executar

### 1. Iniciar Infraestrutura
```bash
# Subir todos os serviÃ§os
docker-compose up -d

# Aguardar inicializaÃ§Ã£o (2-3 minutos)
docker-compose logs -f
```

### 2. Configurar Ambiente
```bash
# Executar setup
chmod +x setup.sh
./setup.sh

# Instalar dependÃªncias Python
pip install -r requirements.txt
```

### 3. Iniciar Pipeline
```bash
# Terminal 1: Gerar dados
python producers/iot_data_generator.py

# Terminal 2: Processar com Spark
python consumers/spark_streaming_processor.py
```

## ğŸ“Š Monitoramento

### URLs de Acesso
- **Spark Master UI**: http://localhost:8080
- **Grafana**: http://localhost:3000 (admin/admin)
- **PostgreSQL**: localhost:5432 (postgres/postgres)

### MÃ©tricas DisponÃ­veis
- Temperatura mÃ©dia por localizaÃ§Ã£o
- Alertas de temperatura alta
- Status de bateria dos sensores
- Atividade de usuÃ¡rios por browser
- DuraÃ§Ã£o de sessÃµes

## ğŸ”§ ConfiguraÃ§Ãµes

### Kafka Topics
- `iot_sensors`: Dados de sensores IoT
- `user_activity`: Atividade de usuÃ¡rios

### Spark Streaming
- **Batch Interval**: 5 segundos
- **Watermark**: 10 minutos
- **Window Size**: 5 minutos
- **Checkpoint**: Habilitado para fault tolerance

### Alertas Configurados
- ğŸŒ¡ï¸ **Temperatura Alta**: > 30Â°C
- ğŸ”‹ **Bateria Baixa**: < 20%
- â±ï¸ **SessÃ£o Longa**: > 5 minutos

## ğŸ“ˆ Casos de Uso

1. **Monitoramento IoT**: Sensores industriais em tempo real
2. **Analytics Web**: Comportamento de usuÃ¡rios
3. **DetecÃ§Ã£o de Anomalias**: Alertas automÃ¡ticos
4. **Dashboards Executivos**: KPIs em tempo real

## ğŸ§ª Testes

```bash
# Verificar tÃ³picos Kafka
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Verificar dados no PostgreSQL
docker exec -it postgres psql -U postgres -d streaming_data -c "SELECT COUNT(*) FROM sensor_data;"

# Monitorar logs Spark
docker logs spark-master
```

## ğŸ”„ PrÃ³ximos Passos

- [ ] Implementar Schema Registry
- [ ] Adicionar Kafka Connect
- [ ] Criar alertas no Grafana
- [ ] Implementar ML para detecÃ§Ã£o de anomalias
- [ ] Adicionar testes unitÃ¡rios

## ğŸ“š Tecnologias Demonstradas

- **Stream Processing**: Kafka + Spark Streaming
- **Real-time Analytics**: AgregaÃ§Ãµes com janelas
- **Data Engineering**: ETL em tempo real
- **DevOps**: Docker + containerizaÃ§Ã£o
- **Monitoring**: Grafana + mÃ©tricas

---

Desenvolvido por Ivan de FranÃ§a